{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorized_stride(array, length, stride):\n",
    "    start = 0\n",
    "    max_time = len(array) - length - 1\n",
    "\n",
    "    sub_windows = (\n",
    "            start +\n",
    "            np.expand_dims(np.arange(length), 0) +\n",
    "            # Create a rightmost vector as [0, V, 2V, ...].\n",
    "            np.expand_dims(np.arange(max_time + 1, step=stride), 0).T\n",
    "    )\n",
    "\n",
    "    return array[sub_windows]\n",
    "\n",
    "\n",
    "# arr = np.array([[i] * 10 for i in range(10)])\n",
    "# vectorized_stride(arr, length=4, stride=4)\n",
    "\n",
    "\n",
    "# def add_features(df):\n",
    "#     quotes_list = [\n",
    "#         Quote(d, o, h, l, c, v)\n",
    "#         for d, o, h, l, c, v\n",
    "#         in zip(df['Timestamp'], df['Open'], df['High'], df['Low'], df['Close'], df['Volume_(Currency)'])\n",
    "#     ]\n",
    "#     ...\n",
    "#     return ...\n",
    "\n",
    "\n",
    "def make_standardised_segments(df, segment_len, segment_amp_range, stride):\n",
    "    segments = vectorized_stride(df.to_numpy(), length=segment_len, stride=stride)\n",
    "    scaler = MinMaxScaler(feature_range=segment_amp_range)\n",
    "    return [pd.DataFrame(scaler.fit_transform(segment), columns=df.columns) for segment in segments]\n",
    "\n",
    "\n",
    "# def tokenize(df, amplitude_range, resolution):\n",
    "#     # quantities = np.linspace(0, window_height, resolution)\n",
    "#     interval = amplitude_range / (resolution - 1)\n",
    "#\n",
    "#     df = interval * np.round(df / interval)  # nan ok?\n",
    "#     # df = df.fillna('<NULL>')\n",
    "#     # df[df.isna().any(axis=1)] = '<NULL>'\n",
    "#     df = df.astype(str)  # verify this works\n",
    "#\n",
    "#     return df\n",
    "\n",
    "def make_curriculum(df, window_length, window_range, stride):\n",
    "    return [make_standardised_segments(df[i:], segment_len=window_length, segment_amp_range=window_range, stride=stride)\n",
    "            for i in range(window_length)]\n",
    "\n",
    "\n",
    "def ts_train_test_split(df, test_size, gap_size):\n",
    "    gap_size = int(gap_size)\n",
    "    train_end = int((1 - test_size) * (len(df) - gap_size))\n",
    "    return df[:train_end], df[train_end + gap_size:]\n",
    "\n",
    "\n",
    "def resample(df, freq):\n",
    "    return df.resample(freq).agg({\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        # 'Volume_(BTC)': 'sum',\n",
    "        'Volume_(Currency)': 'sum',\n",
    "        # 'Weighted_Price': 'mean',\n",
    "        # 'Missing': 'sum',\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mydata import load_data\n",
    "\n",
    "raw_data = load_data('btc')\n",
    "raw_data['Timestamp'] = pd.to_datetime(raw_data['Timestamp'], unit='s')\n",
    "raw_data = raw_data.set_index('Timestamp')\n",
    "raw_data = raw_data.reindex(pd.date_range(raw_data.index.min(), raw_data.index.max(), freq='min'), fill_value=np.nan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data['Missing'] = data.isna().any(axis=1).astype(int)\n",
    "data = raw_data.interpolate(method='index')\n",
    "# todo: show how much is interpolated\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume_(Currency)']].astype(float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# window_length = int(5 * 30 * 24 / 4)\n",
    "window_length = 512\n",
    "stride = 16\n",
    "window_range = (-1, 1)\n",
    "\n",
    "train_data, val_data = ts_train_test_split(data, test_size=0.3, gap_size=stride)\n",
    "val_data, test_data = ts_train_test_split(val_data, test_size=0.5, gap_size=stride)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "freq = '15min'\n",
    "train_data = resample(train_data, freq)\n",
    "val_data = resample(val_data, freq)\n",
    "test_data = resample(test_data, freq)\n",
    "# train_data = add_features(train_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# curriculum = []\n",
    "# for i in range(window_size):\n",
    "#     curriculum.append(tokenize(quantise(normalise(segment(data, offset=i))), mask))\n",
    "\n",
    "train_time_period = train_data.index\n",
    "val_time_period = val_data.index\n",
    "test_time_period = test_data.index\n",
    "\n",
    "train_data = np.stack(make_standardised_segments(train_data, window_length, window_range, stride))\n",
    "val_data = np.stack(make_standardised_segments(val_data, window_length, window_range, stride))\n",
    "test_data = np.stack(make_standardised_segments(test_data, window_length, window_range, stride))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(512, 5)),\n",
    "        # tf.keras.layers.Conv1D(\n",
    "        #     filters=16, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        # tf.keras.layers.Conv1D(\n",
    "        #     filters=32, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        # tf.keras.layers.Conv1D(\n",
    "        #     filters=64, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Conv1D(\n",
    "            filters=32, kernel_size=kernel_size, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Conv1D(\n",
    "            filters=16, kernel_size=kernel_size, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "\n",
    "        tf.keras.layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=kernel_size, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=kernel_size, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        # tf.keras.layers.Conv1DTranspose(\n",
    "        #     filters=64, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        # tf.keras.layers.Conv1DTranspose(\n",
    "        #     filters=64, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        # tf.keras.layers.Conv1DTranspose(\n",
    "        #     filters=64, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        # ),\n",
    "        tf.keras.layers.Conv1DTranspose(filters=5, kernel_size=kernel_size, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "#     initial_learning_rate=0.01,\n",
    "#     decay_steps=10000,\n",
    "#     end_learning_rate=0.00001,\n",
    "#     power=1.0,\n",
    "#     cycle=False,\n",
    "#     name=None\n",
    "# )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(val_data, val_data),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = 'trainingLog'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'trainHistoryDict.json'), 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "test_pred = model.predict(test_data)\n",
    "\n",
    "for i, title in enumerate(['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    plt.title(title)\n",
    "    plt.plot(test_data[1000, :, i], label='original')\n",
    "    plt.plot(test_pred[1000, :, i], label='reconstructed')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, title + '.png'))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from components.transformer import TSTransformerAutoEncoder\n",
    "#\n",
    "# autoencoder = TSTransformerAutoEncoder(vocab_size=1000,\n",
    "#                                        input_shape=(512, 5),\n",
    "#                                        d_embedding=64,\n",
    "#                                        d_compressed=51,\n",
    "#                                        n_layers=4,\n",
    "#                                        FFN_units=2048,\n",
    "#                                        n_heads=8,\n",
    "#                                        dropout_rate=0.1)\n",
    "\n",
    "# autoencoder.compile(optimizer='adam', loss='mae')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 6\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(read_dat(block_path)):\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\tradngTools\\block_parser.py:113\u001B[0m, in \u001B[0;36mread_dat\u001B[1;34m(filepath)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mBlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\tradngTools\\blockchain.py:245\u001B[0m, in \u001B[0;36mBlock.from_file\u001B[1;34m(cls, file)\u001B[0m\n\u001B[0;32m    244\u001B[0m magic_bytes \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;241m4\u001B[39m)[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mhex()\n\u001B[1;32m--> 245\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;66;03m# block_header = f.read(80).hex()\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: invalid literal for int() with base 16: ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "Cell \u001B[1;32mIn [3], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from block_parser import read_dat\n",
    "\n",
    "block_path = 'datasets/blocks/blk00000.dat'\n",
    "blocks = []\n",
    "\n",
    "try:\n",
    "    for i, block in enumerate(read_dat(block_path)):\n",
    "        pass\n",
    "except:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
