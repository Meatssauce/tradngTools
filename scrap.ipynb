{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Layer\n",
    "from keras.models import Model, Sequential\n",
    "from transformers import GPT2Tokenizer\n",
    "from Transformer import TSTransformerAutoEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from stock_indicators import indicators\n",
    "from stock_indicators.indicators.common.quote import Quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def vectorized_stride(array, length, stride):\n",
    "    start = 0\n",
    "    max_time = len(array) - length - 1\n",
    "\n",
    "    sub_windows = (\n",
    "            start +\n",
    "            np.expand_dims(np.arange(length), 0) +\n",
    "            # Create a rightmost vector as [0, V, 2V, ...].\n",
    "            np.expand_dims(np.arange(max_time + 1, step=stride), 0).T\n",
    "    )\n",
    "\n",
    "    return array[sub_windows]\n",
    "\n",
    "\n",
    "# arr = np.array([[i] * 10 for i in range(10)])\n",
    "# vectorized_stride(arr, length=4, stride=4)\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    quotes_list = [\n",
    "        Quote(d, o, h, l, c, v)\n",
    "        for d, o, h, l, c, v\n",
    "        in zip(df['Timestamp'], df['Open'], df['High'], df['Low'], df['Close'], df['Volume_(Currency)'])\n",
    "    ]\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "def make_standardised_segments(df, segment_len, segment_amp_range, stride):\n",
    "    segments = vectorized_stride(df.to_numpy(), length=segment_len, stride=stride)\n",
    "    scaler = MinMaxScaler(feature_range=segment_amp_range)\n",
    "    return [pd.DataFrame(scaler.fit_transform(segment), columns=df.columns) for segment in segments]\n",
    "\n",
    "\n",
    "# def tokenize(df, amplitude_range, resolution):\n",
    "#     # quantities = np.linspace(0, window_height, resolution)\n",
    "#     interval = amplitude_range / (resolution - 1)\n",
    "#\n",
    "#     df = interval * np.round(df / interval)  # nan ok?\n",
    "#     # df = df.fillna('<NULL>')\n",
    "#     # df[df.isna().any(axis=1)] = '<NULL>'\n",
    "#     df = df.astype(str)  # verify this works\n",
    "#\n",
    "#     return df\n",
    "\n",
    "def make_curriculum(df, window_length, window_range, stride):\n",
    "    return [make_standardised_segments(df[i:], segment_len=window_length, segment_amp_range=window_range, stride=stride)\n",
    "            for i in range(window_length)]\n",
    "\n",
    "\n",
    "def ts_train_test_split(df, test_size, gap_size):\n",
    "    gap_size = int(gap_size)\n",
    "    train_end = int((1 - test_size) * (len(df) - gap_size))\n",
    "    return df[:train_end], df[train_end + gap_size:]\n",
    "\n",
    "\n",
    "def resample(df, freq):\n",
    "    return df.resample(freq).agg({\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        # 'Volume_(BTC)': 'sum',\n",
    "        'Volume_(Currency)': 'sum',\n",
    "        # 'Weighted_Price': 'mean',\n",
    "        # 'Missing': 'sum',\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', index_col='Timestamp')  # min by min\n",
    "raw_data.index = pd.to_datetime(raw_data.index, unit='s')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "# data['Missing'] = data.isna().any(axis=1).astype(int)\n",
    "data = raw_data.interpolate(method='index')\n",
    "# todo: show how much is interpolated\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume_(Currency)']]\n",
    "\n",
    "window_length = int(5 * 30 * 24 / 4)\n",
    "# stride = int(10 / 25 * window_length)\n",
    "stride = 16\n",
    "window_range = (-1, 1)\n",
    "\n",
    "train_data, val_data = ts_train_test_split(data, test_size=0.3, gap_size=window_length)\n",
    "val_data, test_data = ts_train_test_split(val_data, test_size=0.15 * 0.3, gap_size=stride)\n",
    "\n",
    "freq = '15min'\n",
    "train_data = resample(train_data, freq)\n",
    "val_data = resample(val_data, freq)\n",
    "test_data = resample(test_data, freq)\n",
    "# train_data = add_features(train_data)\n",
    "\n",
    "# curriculum = []\n",
    "# for i in range(window_size):\n",
    "#     curriculum.append(tokenize(quantise(normalise(segment(data, offset=i))), mask))\n",
    "train_data = np.stack(make_standardised_segments(train_data, window_length, window_range, stride))\n",
    "val_data = np.stack(make_standardised_segments(val_data, window_length, window_range, stride))\n",
    "test_data = np.stack(make_standardised_segments(test_data, window_length, window_range, stride))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['dec_inputs'].",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [10], line 10\u001B[0m\n\u001B[0;32m      1\u001B[0m autoencoder \u001B[38;5;241m=\u001B[39m TSTransformerAutoEncoder(vocab_size_enc\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m      2\u001B[0m                                        vocab_size_dec\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[0;32m      3\u001B[0m                                        d_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5012\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m                                        n_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m      7\u001B[0m                                        dropout_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m      8\u001B[0m autoencoder\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mautoencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\tradngTools\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\PycharmProjects\\tradngTools\\venv\\lib\\site-packages\\keras\\engine\\training.py:3488\u001B[0m, in \u001B[0;36mModel._check_call_args\u001B[1;34m(self, method_name)\u001B[0m\n\u001B[0;32m   3486\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(positional_args) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m   3487\u001B[0m     extra_args \u001B[38;5;241m=\u001B[39m positional_args[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m-> 3488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3489\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModels passed to `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmethod_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` can only have `training` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand the first argument in `call()` as positional arguments, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3491\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextra_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3492\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['dec_inputs']."
     ]
    }
   ],
   "source": [
    "autoencoder = TSTransformerAutoEncoder(vocab_size_enc=1000,\n",
    "                                       vocab_size_dec=500,\n",
    "                                       d_model=5012,\n",
    "                                       n_layers=4,\n",
    "                                       FFN_units=2048,\n",
    "                                       n_heads=8,\n",
    "                                       dropout_rate=0.1)\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "autoencoder.fit(train_data[:5], train_data[:5], epochs=3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xf9\\xbe\\xb4\\xd9\\x1d\\x01\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00;\\xa3\\xed\\xfdz{\\x12\\xb2z\\xc7,>gv\\x8fa\\x7f\\xc8\\x1b\\xc3\\x88\\x8aQ2:\\x9f\\xb8\\xaaK\\x1e^J)\\xab_I\\xff\\xff\\x00\\x1d\\x1d\\xac+|\\x01\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xffM\\x04\\xff\\xff\\x00\\x1d\\x01\\x04EThe Times 03/Jan/2009 Chancellor on brink of second bailout for banks\\xff\\xff\\xff\\xff\\x01\\x00\\xf2\\x05*\\x01\\x00\\x00\\x00CA\\x04g\\x8a\\xfd\\xb0\\xfeUH'\\x19g\\xf1\\xa6q0\\xb7\\x10\\\\\\xd6\\xa8(\\xe09\\t\\xa6yb\\xe0\\xea\\x1fa\\xde\\xb6I\\xf6\\xbc?L\\xef8\\xc4\\xf3U\\x04\\xe5\\x1e\\xc1\\x12\\xde\\\\8M\\xf7\\xba\\x0b\\x8dW\\x8aLp+k\\xf1\\x1d_\\xac\\x00\\x00\\x00\\x00\\xf9\\xbe\\xb4\\xd9\\xd7\\x00\\x00\\x00\\x01\\x00\\x00\\x00o\\xe2\\x8c\\n\"\n",
      "b'f9beb4d91d0100000100000000000000000000000000000000000000000000000000000000000000000000003ba3edfd7a7b12b27ac72c3e67768f617fc81bc3888a51323a9fb8aa4b1e5e4a29ab5f49ffff001d1dac2b7c0101000000010000000000000000000000000000000000000000000000000000000000000000ffffffff4d04ffff001d0104455468652054696d65732030332f4a616e2f32303039204368616e63656c6c6f72206f6e206272696e6b206f66207365636f6e64206261696c6f757420666f722062616e6b73ffffffff0100f2052a01000000434104678afdb0fe5548271967f1a67130b7105cd6a828e03909a67962e0ea1f61deb649f6bc3f4cef38c4f35504e51ec112de5c384df7ba0b8d578a4c702b6bf11d5fac00000000f9beb4d9d7000000010000006fe28c0a'\n"
     ]
    }
   ],
   "source": [
    "# import binascii\n",
    "#\n",
    "# with open(r\"D:\\Bitcoin\\blocks\\blk00000.dat\", 'rb') as f:\n",
    "#     data = f.readline()\n",
    "#     print(data)\n",
    "#     data = binascii.hexlify(data)\n",
    "#     print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
